{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "65559326e76d2b7841244e1dd6ea448671411b95"
   },
   "source": [
    "# NLP Assignment CheatSheet - N-grams Language Model Basics\n",
    "\n",
    "N-gram language models (LM) are a classical approach to sentence modelling, and text autocompletion.\n",
    "\n",
    "We will use the `nltk` (natural language toolkit) python package. \n",
    "If you want to learn more about this popular module, refer to the [official website](https://www.nltk.org/) ([API reference](https://www.nltk.org/api/nltk.html), [installation guide](https://www.nltk.org/install.html)).\n",
    "\n",
    "In particular, the `nltk.lm` submodule provides optimized implementations of classical n-grams language models such as the maximum likelihood estimator (MLE) and its smoothing variants (Laplace, Lidstone, ...).\n",
    "\n",
    "To illustrate the ngram approach, in the NLP assignment, we will apply it on the Trump Tweets dataset, and try to generate new tweets!\n",
    "\n",
    "Before that, this notebook presents the basics on how to preprocess the text data into tokens and ngrams, and to fit LMs with `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b213fb0dd5534ce82d6f1c716e9695ba5cf9758"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First download some nltk resources\n",
    "# (By default '!pip install nltk' does not actually download every resource in the module,\n",
    "# as for example some language models are heavy.)\n",
    "# The following command should download every resource needed for this practical:\n",
    "nltk.download('popular', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: preprocessing and n-grams with dummy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we consider the dummy corpus `corp` with two tokenized documents (sequences of tokens). The tokens are here simple letters, but we can think of them as representing words in our vocabulary. (We have seen how to tokenize raw text in the previous seminar.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c1716261478af5e75bba7799bfefc10bd1ea4ea"
   },
   "outputs": [],
   "source": [
    "corp = [['a', 'b', 'c'], ['a', 'c', 'd', 'c', 'e', 'f']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "800f7632b6a834d36b95211a912b390fcb9dc11b"
   },
   "source": [
    "If we want to train a bigram model, we need to turn this tokenized text into n-grams. We can use the `bigrams` and `ngrams` functions from NLTK as helpers, to turn the each token list document into an ngram list (for ex. with $n=2$ and $n=3$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a5a1c775bab7cf9c67656d4349d8bfca02a80738"
   },
   "outputs": [],
   "source": [
    "list(bigrams(corp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0219988318e39dd0576913b8087af8b35cbdab2f"
   },
   "outputs": [],
   "source": [
    "list(ngrams(corp[1], n=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e3a92b25dc25f3ae86fd265880b7f410e981c670"
   },
   "source": [
    "*Remark:* The `list()` is here just used to display the results, as `bigrams`, `ngrams` and other `nltk` functions return python lazy generators, for efficiency.\n",
    "\n",
    "Notice how \"b\" occurs both as the first and second member of different bigrams but \"a\" and \"c\" don't? \n",
    "\n",
    "It would be nice to indicate to the model how often sentences start with \"a\" and end with \"c\" for example, when we will count those ngrams later-on.\n",
    "\n",
    "\n",
    "A standard way to deal with this is to add special \"padding\" symbols to the document/sequence before splitting it into ngrams. Fortunately, NLTK also has a `pad_sequence` function for that. We use `\"<s>\"` and `\"</s>\"` by convention in `nltk` to pad before and after the sequence, respectively.\n",
    "\n",
    "Lets add the relevent paddings and construct the bigrams and 3-grams for the first text sequence. Note the `n` argument, that tells the function we need padding for `n`-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1ec58c94f58429c9160b4496c939f211f88ade54"
   },
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1ec58c94f58429c9160b4496c939f211f88ade54"
   },
   "outputs": [],
   "source": [
    "#n=2\n",
    "padded_seq2 = list(pad_sequence(corp[0],\n",
    "                                pad_left=True, left_pad_symbol=\"<s>\",\n",
    "                                pad_right=True, right_pad_symbol=\"</s>\",\n",
    "                                n=2)) # The n order of n-grams, if it's 2-grams, you pad once, 3-grams pad twice, etc. \n",
    "padded_seq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ae38644238aa46ac100381c0de046972090b093b"
   },
   "outputs": [],
   "source": [
    "list(ngrams(padded_seq2, n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4cda3fcbd1b8ab48431240e04168011953ebc913"
   },
   "outputs": [],
   "source": [
    "#n=3\n",
    "padded_seq3 = list(pad_sequence(corp[0],\n",
    "                                pad_left=True, left_pad_symbol=\"<s>\",\n",
    "                                pad_right=True, right_pad_symbol=\"</s>\",\n",
    "                                n=3)) # The n order of n-grams, if it's 2-grams, you pad once, 3-grams pad twice, etc. \n",
    "padded_seq3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd572a57371a716471d156144e7ebc136072de01"
   },
   "outputs": [],
   "source": [
    "list(ngrams(padded_seq3, n=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4d80e5707a2efa7ea5cee601d07bca23bc0ae87"
   },
   "source": [
    "Passing all these parameters every time can be tedious and in most cases one uses the same defaults anyway.\n",
    "\n",
    "Thus the `nltk.lm` module provides a convenience function that has all these arguments already set while the other arguments remain the same as for `pad_sequence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb64c3c2a205c5fc5e1fce6f63c3e9038f0d8c4a"
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb64c3c2a205c5fc5e1fce6f63c3e9038f0d8c4a"
   },
   "outputs": [],
   "source": [
    "list(pad_both_ends(corp[0], n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2d2581fae73d38d623a77b5d523c957df2dc8478"
   },
   "source": [
    "Combining the two parts discussed so far we get the following preparation steps for one sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1168917cc340d400f6dba9b2703561785481d8e4"
   },
   "outputs": [],
   "source": [
    "list(bigrams(pad_both_ends(corp[0], n=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2778a5e5b8d95bc395ba41af491818140c7f680e"
   },
   "source": [
    "For versatility and conditional probability computations, the `nltk.lm` n-gram models that we will use typically rely on counting everygrams of order n. \n",
    "For example, LMs of order 2 are trained by counting unigrams (single words) as well as bigrams (word pairs). For LMs of order 3, they usually rely on counting unigrams, bigrams and 3-grams. And so on... \n",
    "That way, an `nltk` LM model of order $n$ can output word probabilities for contexts (i.e. previous words/tokens in the conditioning) of size $0, 1, 2, ..., n-1$ tokens.\n",
    "\n",
    "To construct those everygrams, that will serve as training data for the LM model to count, NLTK once again helpfully provides a function called `everygrams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dfdc9effca718a7881f1fdc4562cdfb29164b96c"
   },
   "outputs": [],
   "source": [
    "from nltk.util import everygrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dfdc9effca718a7881f1fdc4562cdfb29164b96c"
   },
   "outputs": [],
   "source": [
    "padded_seq2 = list(pad_both_ends(corp[0], n=2))\n",
    "\n",
    "list(everygrams(padded_seq2, max_len=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d74b0ba683b733d5a83756361a905625a841e68d"
   },
   "source": [
    "We are almost ready to start counting ngrams, just one more step left.\n",
    "\n",
    "During training and evaluation our model will rely on a vocabulary that defines which words are \"known\" to the model, to efficiently perform the counting.\n",
    "\n",
    "One can create this vocabulary we need to pad our sentences (just like for counting ngrams) and then combine the sentences into one flat stream of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "310c937ea7cc847a26f6c3d08c7169e8cd6b354a"
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "310c937ea7cc847a26f6c3d08c7169e8cd6b354a"
   },
   "outputs": [],
   "source": [
    "list(flatten(pad_both_ends(sent, n=2) for sent in corp)) #vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5bab7d656c474d945669e6a86bf4beec3a44189e"
   },
   "source": [
    "Now that we discussed the necessary preprocessing steps, in most cases, one typically wants to use the same text as the source for both vocabulary and ngram counts.\n",
    "\n",
    "To this aim, the `padded_everygram_pipeline` function does exactly everything above (padding, everygrams, vocabulary stream) for us for the whole tokenized corpus, in a single function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "147ccdd961ac97492ee9f0eadf3dfce7325c6790"
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "147ccdd961ac97492ee9f0eadf3dfce7325c6790"
   },
   "outputs": [],
   "source": [
    "training_neverygrams, padded_vocab_stream = padded_everygram_pipeline(order=2, text=corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b1b15504f2fb4487deec19de1c5ca786e49a42d6"
   },
   "source": [
    "To avoid re-creating the text in memory, both `training_neverygrams` and `padded_vocab_stream` are lazy iterators. They are evaluated on demand at training time.\n",
    "\n",
    "For the sake of understanding the outputs of `padded_everygram_pipeline`, we \"materialize\" the lazy iterators by casting them into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa18a8a4a700926c30488c94cf519d8900de8ddf"
   },
   "outputs": [],
   "source": [
    "training_neverygrams, padded_vocab_stream = padded_everygram_pipeline(2, corp)\n",
    "\n",
    "print('==== n-everygram data (n=2) for each sequence in \"corp\": ====')\n",
    "for ngramlize_sent in training_neverygrams:\n",
    "    print(list(ngramlize_sent))\n",
    "    print()\n",
    "print('==== Vocabulary data: ====')\n",
    "print(list(padded_vocab_stream))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Language models in NLTK: Basic usage tips\n",
    "\n",
    "The `nltk.lm` submodule has implementations of the language models (LM) you have seen in class, and several others. In particular, you will find implementations of: The simple Maximum Likelihood Estimator (MLE) (`nltk.lm.MLE`), Laplace smoothing (`nltk.lm.Laplace`), and Lidstone smoothing (`nltk.lm.Lidstone`). \n",
    "Lidstone is simply a generalization of Laplace, where a user-selected value $\\gamma$ is added to the counts, instead of the value $1$ added with Laplace.\n",
    "\n",
    "In this section, you will find the very basics on how to use these language model implementations. For more details, you are encouraged to look into the nltk doccumentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Fitting an NLTK LM\n",
    "\n",
    "The LM model usage, is quite similar to scikit-learn. With the simple MLE as an example, it is first instantiated (there might be some other hyperparameters for other models):\n",
    "```python\n",
    "    from nltk.lm import MLE\n",
    "    model = MLE(order = n) # n is the desired order of the model\n",
    "```\n",
    "Then it is fit to a training corpus that has been properly preprocessed, into everygrams and a vocabulary stream, for the correct order $n$:\n",
    "```python\n",
    "    model.fit(text = training_neverygrams, vocabulary_text = padded_vocab_stream)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Accessing the fitted model\n",
    "\n",
    "Initializing the MLE model, creates an empty vocabulary, which gets filled as we fit the model. The vocabulary object is accessible as an argument. Try for example what the following do:\n",
    "```python\n",
    "    model.vocab\n",
    "    model.vocab.lookup(token_list)\n",
    "```\n",
    "\n",
    "For a more advanced usage, the vocabulary can be constructed separately and given to the model, instead of letting it infer it from the vocabulary stream. This allows for example cutting-off infrequent words from the vocabulary. If you are interested in the implementation and going a bit further, you can check out the documentation for the `nltk.lm.vocabulary.Vocabulary` class [here](https://www.nltk.org/api/nltk.lm.vocabulary.html) or the source code: [`nltk.lm.vocabulary.Vocabulary`](https://github.com/nltk/nltk/blob/develop/nltk/lm/vocabulary.py).\n",
    "\n",
    "Then, fitting n-gram LMs basically boils down to counting the number of word/token and n-gram occurrences in the training data. To access token counts, and conditional token counts (in a context of one or several preceding tokens), try:\n",
    "```python\n",
    "    model.counts\n",
    "    model.counts['word']\n",
    "    model.counts[('context_word1', \"context_word2\", ...)][\"word\"]\n",
    "```\n",
    "However, the real purpose of training a language model is to have it score how probable words are in certain contexts. \n",
    "For the MLE, the model returns the item's relative frequency as its score, i.e. (conditional) occurrence probability.\n",
    "```python\n",
    "    model.score('word')                                             # P('word')\n",
    "    model.score('word', ('context_word1', \"context_word2\", ...))    # P('word'|'context_word1 context_word2 ...')\n",
    "```\n",
    "To avoid underflow when working with many small score values it makes sense to take their logarithm. \n",
    "For convenience this can be done by using the `logscore` method instead of the `score`.\n",
    "```python\n",
    "    model.logscore('word')\n",
    "    model.logscore('word', ('context_word1', \"context_word2\", ...))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Generation with NLTK LMs\n",
    "\n",
    "One cool feature of ngram models is that they can be used to generate text. The `nltk.lm.model` classes have a `.generate()` method to sample sequentially from the estimated (conditional) probabilities. This can be achieved using:\n",
    "```python\n",
    "    model.generate(num_words = num_words, text_seed = initial_context_tokens, random_seed = None)\n",
    "```\n",
    "Keep in mind that this will generate `num_words` new words according to the model's fitted scores, as a list of vocabulary tokens. For a realistic output text, it might thus need some post-processing. `nltk.tokenize.treebank.TreebankWordDetokenizer()` provides a general-purpose **sentence** detokenizer, but might need some additional post-processing for specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Perplexity\n",
    "\n",
    "The model perplexity is a normalized form of the sequence probability, as seen in the lecture. It can be used on a kept-aside test dataset to evaluate the performance of a ngram probability model. The `nltk.lm.model` classes have a `.perplexity()` method to compute the perplexity on a given list or corpus of n-grams.\n",
    "```python\n",
    "    model.perplexity(test_ngrams)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
